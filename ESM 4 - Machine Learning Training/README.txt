README.txt
==========

Title
-----
Machine Learning-Based IoT Encryption Algorithm Selection System

Manuscript Context
------------------
This archive accompanies the machine learning component of the IoT encryption study.  
It provides a complete JupyterLab workflow for training, optimizing, and evaluating predictive models that determine the most suitable encryption algorithm for an IoT device based on performance, energy, and memory metrics.

Contents
--------
Notebook files
- ML Model/ML Models.ipynb  
  Main workflow notebook for data preparation, feature engineering, model training, evaluation, and optimization.
- ML Model/Figures_Recreation.ipynb
  Supporting notebook used to reproduce figures from the main workflow notebook in accordance with the Springer   
  journal formatting guidelines.

Input files (generated from previous stages)
- Data_Analysis/Standardized/standardized_results_{DATA_SIZE}_{weight_strategy}.csv  
- Data_Analysis/Summary/summary_best_config_{DATA_SIZE}_{weight_strategy}.csv  

These CSVs are **produced by the IoT algorithm performance analysis stage**
(`Data_Analysis_Performance.ipynb` or `Data_Analysis_Performance.py`)  
and serve as the **input dataset** for training and evaluating the machine learning models.

Local utilities (modules)
- Tools_ml_app/ml_utils.py  
  Utility functions for data preprocessing (AlignColumns, FrozenPreprocessor, EnsureDataFrame, SelectColumnsByName).
- mylib/mylib.py  
  Helper functions for visualization, evaluation, and bias–variance metrics (show_labels_dist, show_metrics, bias_var_metrics1, safe, plot_models_micro_roc, plot_confusion_after, get_feature_names).

Generated model folders
- ML Model/models_set1_baseline/  
- ML Model/models_set1_sfs_backward_pipeline/  
- ML Model/models_set1_smote_pipeline/  
- ML Model/models_set1_sfs_smote_pipeline/  
- ML Model/models_set1_hyperparameter/  
- ML Model/models_set1_hyperparameter_SBS/  
- ML Model/models_set1_hyperparameter_SMOTE/  
- ML Model/models_set1_hyperparameter_SBS_SMOTE/  

Environment descriptors
- ML Model/generate_env_info_auto.py        (utility to create a concise env report)
- ML Model/appendix_environment_concise.txt (generated environment summary for reproducibility)
- requirements.txt                          (exact Python package versions for the ML stage)

To regenerate the environment file:
  python "ML Model/generate_env_info_auto.py" --source "ML Model" --out "ML Model/appendix_environment_concise.txt"

System Requirements
-------------------
Python version: 3.13.2  
Supported OS tested: Windows 11 (AMD64)  
Runtime environment: JupyterLab  

Python Packages (pinning in requirements.txt)
---------------------------------------------
numpy  
pandas  
scikit-learn  
matplotlib  
joblib  
mlxtend  
imbalanced-learn  
yellowbrick  
packaging  
pathlib  
IPython  
gc  
time  

Notes:
- scikit-learn used for preprocessing, pipelines, and classifiers.  
- mlxtend used for sequential feature selection (SFS/SBS).  
- imbalanced-learn provides SMOTE and ImbPipeline integration.  
- yellowbrick used for visualization of class balance and model performance.  
- joblib handles model serialization for reproducibility.

Installation
------------
Create and activate a virtual environment (recommended):
  python -m venv .venv
  .venv\Scripts\activate   (Windows)
  # source .venv/bin/activate   (Linux/macOS)

Install required packages:
  pip install -r requirements.txt

Ensure supporting modules are importable by preserving the provided folder structure:
  project_root/
    ML Model/ML Models.ipynb
    Tools_ml_app/ml_utils.py
    mylib/mylib.py

If paths are relocated, run from project_root or add project_root to PYTHONPATH.

Project Layout (required directories)
-------------------------------------
project_root/
  Data_Simulation/
    Simulation Results/
      IoT_Encryption_Decryption_Simulation_Results_{DATA_SIZE}.csv             # <- generated by Data_Generation_Final.py
  Data_Analysis/
    Standardized/
      standardized_results_{DATA_SIZE}_{weight_strategy}.csv                    # <- input to ML stage
    Summary/
      summary_best_config_{DATA_SIZE}_{weight_strategy}.csv                     # <- input to ML stage
  ML Model/
    ML Models.ipynb                                                             # <- this notebook
    generate_env_info_auto.py                                                   # <- env reporter
    appendix_environment_concise.txt                                            # <- generated env snapshot
    models_set1_baseline/
    models_set1_sfs_backward_pipeline/
    models_set1_smote_pipeline/
    models_set1_sfs_smote_pipeline/
    models_set1_hyperparameter/
    models_set1_hyperparameter_SBS/
    models_set1_hyperparameter_SMOTE/
    models_set1_hyperparameter_SBS_SMOTE/
  Tools_ml_app/
    ml_utils.py
  mylib/
    mylib.py

If paths differ, update the CSV read paths in the notebook cells accordingly.

Workflow Overview
-----------------
1. Data Collection and Formation  
   - Import and merge IoT performance datasets (duration, RAM, ROM, battery metrics).  
   - Add priority fields for each metric and concatenate across data sizes.  
   - Filter valid algorithm results and drop intermediate columns.

2. Data Preprocessing  
   - Handle missing and redundant columns.  
   - Separate features (X) and labels (y).  
   - Detect categorical and numerical features.  
   - Apply one-hot encoding and normalization via ColumnTransformer.  
   - Split data using stratified sampling (test size = 20%).

3. Baseline Model Training  
   - Train Logistic Regression, SVC, KNN, Random Forest, and MLP pipelines.  
   - Evaluate with accuracy, precision, recall, F1, MCC, and confusion matrices.  
   - Save fitted pipelines to `/models_set1_baseline/`.

4. Bias–Variance Decomposition  
   - Use 5-fold cross-validation with bias_var_metrics1() to compute bias², variance, and expected error.  
   - Visualize comparative error composition per model.

5. Sequential Feature Selection (SFS/SBS)  
   - Apply backward floating feature selection using Random Forest and roc_auc_ovr scoring.  
   - Save selected subsets and refitted pipelines to `/models_set1_sfs_backward_pipeline/`.

6. Resampling (SMOTE)  
   - Apply Synthetic Minority Over-sampling Technique to balance training data.  
   - Visualize class distribution before and after oversampling.  
   - Save fitted models to `/models_set1_smote_pipeline/`.

7. Hyperparameter Tuning  
   - Use GridSearchCV (5-fold CV, scoring = MCC) for Random Forest optimization.  
   - Parameter grid includes n_estimators, max_depth, criterion, and feature constraints.  
   - Save tuned pipelines to `/models_set1_hyperparameter*/`.

8. Combined Optimizations  
   - Evaluate combinations: Baseline, Sequential, SMOTE, Sequential+SMOTE,  
     Hyperparameter, Hyperparameter_SBS, Hyperparameter_SMOTE, and Hyperparameter_SBS_SMOTE.  
   - Compare via bias–variance, ROC–AUC, and confusion matrix plots.

9. Visualization and Reporting  
   - Generate ROC–AUC curves (micro-averaged).  
   - Render confusion matrices for each model.  
   - Plot class balance pre/post SMOTE.  
   - Display bar charts for GridSearchCV top parameters.  
   - Summarize metrics and runtime comparisons.

Outputs and Structure
---------------------
All trained and optimized models are saved as serialized `.pkl` files within the corresponding subfolders under `ML Model/`:
  /models_set1_baseline/
  /models_set1_sfs_backward_pipeline/
  /models_set1_smote_pipeline/
  /models_set1_sfs_smote_pipeline/
  /models_set1_hyperparameter/
  /models_set1_hyperparameter_SBS/
  /models_set1_hyperparameter_SMOTE/
  /models_set1_hyperparameter_SBS_SMOTE/

Each pipeline includes preprocessing and classifier components, ensuring consistent inference on unseen data.

Key Custom Functions
--------------------
From mylib/mylib.py:
  - show_labels_dist() – displays class label proportions.  
  - show_metrics() – prints accuracy, precision, recall, F1, MCC.  
  - bias_var_metrics1() – computes bias–variance decomposition.  
  - safe() – sanitizes filenames for saving.  
  - plot_models_micro_roc() – plots comparative ROC curves.  
  - plot_confusion_after() – displays confusion matrices.  
  - get_feature_names() – extracts processed feature names.

From Tools_ml_app/ml_utils.py:
  - AlignColumns, FrozenPreprocessor, EnsureDataFrame, SelectColumnsByName –  
    maintain consistent feature alignment across transformations.

Security and Scope
------------------
- Intended for research and academic replication.  
- Not optimized or validated for production use.  
- Random seeds fixed for reproducibility.  
- SMOTE applied only to training partitions (no data leakage).  
- All visualization routines require an active Jupyter backend.  
- Compatible with CPU-based training (no GPU dependency).

Reproducibility Checklist
-------------------------
- Python: 3.13.2  
- requirements.txt installed in a fresh virtual environment  
- Input CSVs from the analysis stage available under Data_Analysis/Standardized and Data_Analysis/Summary  
- Folder structure preserved (Tools_ml_app, mylib, and ML Model in project_root)  
- ML Model/appendix_environment_concise.txt included for runtime provenance  
  (auto-generated by ML Model/generate_env_info_auto.py)  
- Jupyter notebook executed sequentially without skipping cells  

Known Limitations
-----------------
- Results depend on dataset version consistency.  
- Feature subsets selected via SBS may vary slightly by random seed.  
- Some classifiers (e.g., MLP) may have non-deterministic initialization.  
- Visual plots require sufficient display resolution in Jupyter.  

License and Third-Party Notices
-------------------------------
- This code is distributed for peer review and academic reproducibility.  
- scikit-learn, imbalanced-learn, mlxtend, and yellowbrick are licensed under BSD or compatible terms.  
- joblib and matplotlib follow their respective open-source licenses.  

Contact
-------
Corresponding author: NAME, AFFILIATION, EMAIL  
Version/date of this ESM: YYYY-MM-DD
